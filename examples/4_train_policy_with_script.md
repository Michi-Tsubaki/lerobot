# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®èª¬æ˜Žã€ãã®ä½¿ç”¨æ–¹æ³•ã€ç‰¹ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œã«å¿…è¦ãªã™ã¹ã¦ã®è¨­å®šæ–¹æ³•ã«ã¤ã„ã¦èª¬æ˜Žã—ã¾ã™ã€‚

> **æ³¨æ„:** ä»¥ä¸‹ã®èª¬æ˜Žã¯ã€CUDA GPUã‚’æ­è¼‰ã—ãŸãƒžã‚·ãƒ³ã§ã‚³ãƒžãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚GPUãŒãªã„å ´åˆï¼ˆã¾ãŸã¯Macã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆï¼‰ã€`--device=cpu`ï¼ˆã¾ãŸã¯ãã‚Œãžã‚Œ`--device=mps`ï¼‰ã‚’è¿½åŠ ã§ãã¾ã™ã€‚ãŸã ã—ã€CPUã§ã¯å®Ÿè¡Œé€Ÿåº¦ãŒå¤§å¹…ã«é…ããªã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

LeRobotã¯[`lerobot/scripts/train.py`](../../lerobot/scripts/train.py)ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚æ¦‚è¦ã¨ã—ã¦ã€ä»¥ä¸‹ã®ã“ã¨ã‚’è¡Œã„ã¾ã™ï¼š

- ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ãŸã‚ã®è¨­å®šã‚’åˆæœŸåŒ–/ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¾ã™ã€‚
- ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ãã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾å¿œã™ã‚‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¾ã™ã€‚
- ãƒãƒªã‚·ãƒ¼ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¾ã™ã€‚
- é †ä¼æ’­ã€é€†ä¼æ’­ã€æœ€é©åŒ–ã‚¹ãƒ†ãƒƒãƒ—ã€ãŠã‚ˆã³å®šæœŸçš„ãªãƒ­ã‚°è¨˜éŒ²ã€è©•ä¾¡ï¼ˆç’°å¢ƒä¸Šã§ã®ãƒãƒªã‚·ãƒ¼ã®è©•ä¾¡ï¼‰ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å«ã‚€æ¨™æº–çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

## è¨­å®šã‚·ã‚¹ãƒ†ãƒ ã®æ¦‚è¦

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã¯ã€ãƒ¡ã‚¤ãƒ³é–¢æ•°`train`ã¯`TrainPipelineConfig`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æœŸå¾…ã—ã¾ã™ï¼š
```python
# train.py
@parser.wrap()
def train(cfg: TrainPipelineConfig):
```

[`lerobot/configs/train.py`](../../lerobot/configs/train.py)ã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹`TrainPipelineConfig`ã‚’ç¢ºèªã§ãã¾ã™ï¼ˆã“ã‚Œã¯è©³ç´°ã«ã‚³ãƒ¡ãƒ³ãƒˆãŒä»˜ã‘ã‚‰ã‚Œã¦ãŠã‚Šã€ã™ã¹ã¦ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã¨ãªã‚‹ã“ã¨ã‚’æ„å›³ã—ã¦ã„ã¾ã™ï¼‰

ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã™ã‚‹éš›ã€ã‚³ãƒžãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰ã®å…¥åŠ›ã¯`@parser.wrap()`ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã«ã‚ˆã£ã¦è§£æžã•ã‚Œã€ã“ã®ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒè‡ªå‹•çš„ã«ç”Ÿæˆã•ã‚Œã¾ã™ã€‚å†…éƒ¨çš„ã«ã¯ã€ã“ã‚Œã¯[Draccus](https://github.com/dlwh/draccus)ã¨ã„ã†ã“ã®ç›®çš„ã®ãŸã‚ã«ä½œã‚‰ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã«ã‚ˆã£ã¦è¡Œã‚ã‚Œã¾ã™ã€‚Hydraã«æ…£ã‚Œã¦ã„ã‚‹æ–¹ãªã‚‰ã€Draccusã‚‚åŒæ§˜ã«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.jsonã€.yamlï¼‰ã‹ã‚‰è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ã•ã‚‰ã«ã‚³ãƒžãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å…¥åŠ›ã‚’é€šã˜ã¦ãã‚Œã‚‰ã®å€¤ã‚’ä¸Šæ›¸ãã§ãã¾ã™ã€‚Hydraã¨ã¯ç•°ãªã‚Šã€ã“ã‚Œã‚‰ã®è¨­å®šã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§å®Œå…¨ã«å®šç¾©ã•ã‚Œã‚‹ã®ã§ã¯ãªãã€dataclassã‚’é€šã˜ã¦ã‚³ãƒ¼ãƒ‰ã§äº‹å‰å®šç¾©ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚ˆã‚ŠåŽ³å¯†ãªã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º/ãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã€åž‹ä»˜ã‘ã€ãŠã‚ˆã³ã‚³ãƒ¼ãƒ‰å†…ã§è¨­å®šã‚’ç›´æŽ¥ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦æ“ä½œã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ï¼ˆã“ã‚Œã«ã‚ˆã‚Šã€IDEã§ã®ã‚ªãƒ¼ãƒˆã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆã€å®šç¾©ã¸ã®ã‚¸ãƒ£ãƒ³ãƒ—ãªã©ã®ä¾¿åˆ©ãªæ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã™ï¼‰ã€‚

ç°¡ç•¥åŒ–ã—ãŸä¾‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã«ã¯ã€ä»–ã®å±žæ€§ã®ä¸­ã§ã‚‚ä»¥ä¸‹ã®ã‚ˆã†ãªå±žæ€§ãŒã‚ã‚Šã¾ã™ï¼š
```python
@dataclass
class TrainPipelineConfig:
    dataset: DatasetConfig
    env: envs.EnvConfig | None = None
    policy: PreTrainedConfig | None = None
```
ã“ã“ã§ã€ä¾‹ãˆã°`DatasetConfig`ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã¦ã„ã¾ã™ï¼š
```python
@dataclass
class DatasetConfig:
    repo_id: str
    episodes: list[int] | None = None
    video_backend: str = "pyav"
```

ã“ã‚Œã«ã‚ˆã‚Šã€ä¾‹ãˆã°`TrainPipelineConfig`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹`cfg`ãŒã‚ã‚‹å ´åˆã€`cfg.dataset.repo_id`ã§`repo_id`ã®å€¤ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã¨ã„ã†éšŽå±¤é–¢ä¿‚ãŒä½œæˆã•ã‚Œã¾ã™ã€‚
ã‚³ãƒžãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰ã€éžå¸¸ã«ä¼¼ãŸæ§‹æ–‡`--dataset.repo_id=repo/id`ã‚’ä½¿ç”¨ã—ã¦ã“ã®å€¤ã‚’æŒ‡å®šã§ãã¾ã™ã€‚

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ã™ã¹ã¦ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯dataclassã§æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’å–ã‚Šã¾ã™ã€‚ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãŒãªã„å ´åˆã€ã‚³ãƒžãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã¾ãŸã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚‚ã‚³ãƒžãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§æŒ‡å®šã—ã¾ã™ - è©³ç´°ã¯å¾Œè¿°ï¼‰ã€‚ä¸Šè¨˜ã®ä¾‹ã§ã¯ã€`dataset`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãŒãªã„ãŸã‚ã€æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

## CLIã‹ã‚‰å€¤ã‚’æŒ‡å®šã™ã‚‹

[Diffusion Policy](../../lerobot/common/policies/diffusion)ã‚’[pusht](https://huggingface.co/datasets/lerobot/pusht)ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¨“ç·´ã—ã€è©•ä¾¡ã®ãŸã‚ã«[gym_pusht](https://github.com/huggingface/gym-pusht)ç’°å¢ƒã‚’ä½¿ç”¨ã—ãŸã„ã¨ã—ã¾ã—ã‚‡ã†ã€‚ãã®ãŸã‚ã®ã‚³ãƒžãƒ³ãƒ‰ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š
```bash
python lerobot/scripts/train.py \
    --dataset.repo_id=lerobot/pusht \
    --policy.type=diffusion \
    --env.type=pusht
```

ã“ã‚Œã‚’åˆ†è§£ã—ã¦èª¬æ˜Žã—ã¾ã—ã‚‡ã†ï¼š
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æŒ‡å®šã™ã‚‹ã«ã¯ã€`DatasetConfig`ã§å”¯ä¸€å¿…é ˆã®å¼•æ•°ã§ã‚ã‚‹ãƒãƒ–ä¸Šã®`repo_id`ã‚’æŒ‡å®šã™ã‚‹ã ã‘ã§è‰¯ã„ã§ã™ã€‚æ®‹ã‚Šã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãŒã‚ã‚Šã€ã“ã®å ´åˆã¯ãã‚Œã‚‰ã§å•é¡Œãªã„ãŸã‚ã€å˜ã«`--dataset.repo_id=lerobot/pusht`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã™ã‚‹ã ã‘ã§ã™ã€‚
- ãƒãƒªã‚·ãƒ¼ã‚’æŒ‡å®šã™ã‚‹ã«ã¯ã€`--policy`ã«`.type`ã‚’ä»˜ã‘ã¦diffusionãƒãƒªã‚·ãƒ¼ã‚’é¸æŠžã™ã‚‹ã ã‘ã§ã™ã€‚ã“ã“ã§ã€`.type`ã¯`draccus.ChoiceRegistry`ã‚’ç¶™æ‰¿ã—ã€`register_subclass()`ãƒ¡ã‚½ãƒƒãƒ‰ã§ãƒ‡ã‚³ãƒ¬ãƒ¼ãƒˆã•ã‚ŒãŸè¨­å®šã‚¯ãƒ©ã‚¹ã‚’é¸æŠžã§ãã‚‹ç‰¹åˆ¥ãªå¼•æ•°ã§ã™ã€‚ã“ã®æ©Ÿèƒ½ã®è©³ç´°ãªèª¬æ˜Žã«ã¤ã„ã¦ã¯ã€ã“ã®[Draccusãƒ‡ãƒ¢](https://github.com/dlwh/draccus?tab=readme-ov-file#more-flexible-configuration-with-choice-types)ã‚’ã”è¦§ãã ã•ã„ã€‚ç§ãŸã¡ã®ã‚³ãƒ¼ãƒ‰ã§ã¯ã€ã“ã®ä»•çµ„ã¿ã‚’ä¸»ã«ãƒãƒªã‚·ãƒ¼ã€ç’°å¢ƒã€ãƒ­ãƒœãƒƒãƒˆã€ãŠã‚ˆã³ã‚ªãƒ—ãƒ†ã‚£ãƒžã‚¤ã‚¶ãªã©ã®ä»–ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®é¸æŠžã«ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚é¸æŠžå¯èƒ½ãªãƒãƒªã‚·ãƒ¼ã¯[lerobot/common/policies](../../lerobot/common/policies)ã«ã‚ã‚Šã¾ã™ã€‚
- åŒæ§˜ã«ã€`--env.type=pusht`ã§ç’°å¢ƒã‚’é¸æŠžã—ã¾ã™ã€‚åˆ©ç”¨å¯èƒ½ãªç’°å¢ƒè¨­å®šã¯[`lerobot/common/envs/configs.py`](../../lerobot/common/envs/configs.py)ã«ã‚ã‚Šã¾ã™ã€‚

åˆ¥ã®ä¾‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚[ACT](../../lerobot/common/policies/act)ã‚’[lerobot/aloha_sim_insertion_human](https://huggingface.co/datasets/lerobot/aloha_sim_insertion_human)ã§è¨“ç·´ã—ã€è©•ä¾¡ã®ãŸã‚ã«[gym-aloha](https://github.com/huggingface/gym-aloha)ç’°å¢ƒã‚’ä½¿ç”¨ã—ã¦ã„ãŸã¨ã—ã¾ã™ï¼š
```bash
python lerobot/scripts/train.py \
    --policy.type=act \
    --dataset.repo_id=lerobot/aloha_sim_insertion_human \
    --env.type=aloha \
    --output_dir=outputs/train/act_aloha_insertion
```
> `--output_dir`ã‚’è¿½åŠ ã—ã¦ã€ã“ã®å®Ÿè¡Œã‹ã‚‰ã®å‡ºåŠ›ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã€è¨­å®šãªã©ï¼‰ã‚’æ›¸ãè¾¼ã‚€å ´æ‰€ã‚’æ˜Žç¤ºçš„ã«æŒ‡å®šã—ãŸã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã¯å¿…é ˆã§ã¯ãªãã€æŒ‡å®šã—ãªã„å ´åˆã¯ã€ç¾åœ¨ã®æ—¥ä»˜ã¨æ™‚åˆ»ã€env.typeã€policy.typeã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒä½œæˆã•ã‚Œã¾ã™ã€‚ã“ã‚Œã¯é€šå¸¸`outputs/train/2025-01-24/16-10-05_aloha_act`ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

ä»Šåº¦ã¯åˆ¥ã®ã‚¿ã‚¹ã‚¯ã§alohaã®åˆ¥ã®ãƒãƒªã‚·ãƒ¼ã‚’è¨“ç·´ã—ãŸã„ã¨ã—ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å¤‰æ›´ã—ã¦ã€ä»£ã‚ã‚Šã«[lerobot/aloha_sim_transfer_cube_human](https://huggingface.co/datasets/lerobot/aloha_sim_transfer_cube_human)ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã‚‚ã¡ã‚ã‚“ã€ã“ã®ã‚¿ã‚¹ã‚¯ã«åˆã‚ã›ã¦ç’°å¢ƒã®ã‚¿ã‚¹ã‚¯ã‚‚å¤‰æ›´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
[`AlohaEnv`](../../lerobot/common/envs/configs.py)è¨­å®šã‚’è¦‹ã‚‹ã¨ã€ã‚¿ã‚¹ã‚¯ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§`"AlohaInsertion-v0"`ã§ã€ã“ã‚Œã¯ä¸Šè¨˜ã®ã‚³ãƒžãƒ³ãƒ‰ã§è¨“ç·´ã—ãŸã‚¿ã‚¹ã‚¯ã«å¯¾å¿œã—ã¾ã™ã€‚[gym-aloha](https://github.com/huggingface/gym-aloha?tab=readme-ov-file#description)ç’°å¢ƒã«ã¯ã€è¨“ç·´ã—ãŸã„åˆ¥ã®ã‚¿ã‚¹ã‚¯ã«å¯¾å¿œã™ã‚‹`AlohaTransferCube-v0`ã‚¿ã‚¹ã‚¯ã‚‚ã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã‚‰ã‚’ã¾ã¨ã‚ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚³ãƒžãƒ³ãƒ‰ã§æ–°ã—ã„ãƒãƒªã‚·ãƒ¼ã‚’ã“ã®ç•°ãªã‚‹ã‚¿ã‚¹ã‚¯ã§è¨“ç·´ã§ãã¾ã™ï¼š
```bash
python lerobot/scripts/train.py \
    --policy.type=act \
    --dataset.repo_id=lerobot/aloha_sim_transfer_cube_human \
    --env.type=aloha \
    --env.task=AlohaTransferCube-v0 \
    --output_dir=outputs/train/act_aloha_transfer
```

## è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ãƒ­ãƒ¼ãƒ‰

ã•ã¦ã€ä¸Šè¨˜ã®å®Ÿè¡Œã‚’å†ç¾ã—ãŸã„ã¨ã—ã¾ã™ã€‚ãã®å®Ÿè¡Œã§ã¯ã€ä½¿ç”¨ã—ãŸ`TrainPipelineConfig`ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã—ãŸ`train_config.json`ãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™ï¼š
```json
{
    "dataset": {
        "repo_id": "lerobot/aloha_sim_transfer_cube_human",
        "episodes": null,
        ...
    },
    "env": {
        "type": "aloha",
        "task": "AlohaTransferCube-v0",
        "fps": 50,
        ...
    },
    "policy": {
        "type": "act",
        "n_obs_steps": 1,
        ...
    },
    ...
}
```

ä»¥ä¸‹ã®ã‚ˆã†ã«ã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šå€¤ã‚’ç°¡å˜ã«ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ï¼š
```bash
python lerobot/scripts/train.py \
    --config_path=outputs/train/act_aloha_transfer/checkpoints/last/pretrained_model/ \
    --output_dir=outputs/train/act_aloha_transfer_2
```
`--config_path`ã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’åˆæœŸåŒ–ã§ãã‚‹ç‰¹åˆ¥ãªå¼•æ•°ã§ã™ã€‚`train_config.json`ã‚’å«ã‚€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã™ã‚‹ã‹ã€è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«è‡ªä½“ã‚’ç›´æŽ¥æŒ‡å®šã§ãã¾ã™ã€‚

Hydraã¨åŒæ§˜ã«ã€å¿…è¦ã«å¿œã˜ã¦CLIã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¸Šæ›¸ãã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ï¼š
```bash
python lerobot/scripts/train.py \
    --config_path=outputs/train/act_aloha_transfer/checkpoints/last/pretrained_model/ \
    --output_dir=outputs/train/act_aloha_transfer_2
    --policy.n_action_steps=80
```
> æ³¨æ„ï¼šä¸€èˆ¬çš„ã«`--output_dir`ã¯å¿…é ˆã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€ã“ã®å ´åˆã¯`train_config.json`ã‹ã‚‰å€¤ã‚’å–å¾—ã™ã‚‹ãŸã‚ï¼ˆå€¤ã¯`outputs/train/act_aloha_transfer`ï¼‰ã€æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ä»¥å‰ã®å®Ÿè¡Œã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª¤ã£ã¦å‰Šé™¤ã™ã‚‹ã“ã¨ã‚’é˜²ããŸã‚ã€æ—¢å­˜ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æ›¸ãè¾¼ã‚‚ã†ã¨ã™ã‚‹ã¨ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ã€‚ã“ã‚Œã¯å®Ÿè¡Œã‚’å†é–‹ã™ã‚‹å ´åˆã«ã¯å½“ã¦ã¯ã¾ã‚Šã¾ã›ã‚“ï¼ˆæ¬¡ã«å­¦ã³ã¾ã™ï¼‰ã€‚

`--config_path`ã¯ã€`train_config.json`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å«ã‚€ãƒãƒ–ä¸Šã®ãƒªãƒã‚¸ãƒˆãƒªã®repo_idã‚‚å—ã‘å…¥ã‚Œã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä¾‹ãˆã°ï¼š
```bash
python lerobot/scripts/train.py --config_path=lerobot/diffusion_pusht
```
ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€[lerobot/diffusion_pusht](https://huggingface.co/lerobot/diffusion_pusht)ã®è¨“ç·´ã«ä½¿ç”¨ã•ã‚ŒãŸã®ã¨åŒã˜è¨­å®šã§è¨“ç·´å®Ÿè¡ŒãŒé–‹å§‹ã•ã‚Œã¾ã™ã€‚

## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å†é–‹

ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã‚„ä¸­æ–­ãªã©ã®ç†ç”±ã§è¨“ç·´å®Ÿè¡Œã‚’å†é–‹ã§ãã‚‹ã“ã¨ã¯é‡è¦ã§ã™ã€‚ã“ã“ã§ãã®æ–¹æ³•ã‚’èª¬æ˜Žã—ã¾ã™ã€‚

å‰å›žã®å®Ÿè¡Œã®ã‚³ãƒžãƒ³ãƒ‰ã‚’å†åˆ©ç”¨ã—ã€ã„ãã¤ã‹ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼š
```bash
python lerobot/scripts/train.py \
    --policy.type=act \
    --dataset.repo_id=lerobot/aloha_sim_transfer_cube_human \
    --env.type=aloha \
    --env.task=AlohaTransferCube-v0 \
    --log_freq=25 \
    --save_freq=100 \
    --output_dir=outputs/train/run_resumption
```

ã“ã“ã§ã¯ã€å†é–‹ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒã§ãã‚‹ã‚ˆã†ã«ã€ãƒ­ã‚°é »åº¦ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé »åº¦ã‚’ä½Žã„æ•°å€¤ã«è¨­å®šã—ã¾ã—ãŸã€‚ï¼ˆãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«ã‚ˆã‚Šã¾ã™ãŒï¼‰1åˆ†ä»¥å†…ã«ãƒ­ã‚°ãŒè¡¨ç¤ºã•ã‚Œã€æœ€åˆã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒä½œæˆã•ã‚Œã‚‹ã¯ãšã§ã™ã€‚æœ€åˆã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒä½œæˆã•ã‚Œã‚‹ã®ã‚’å¾…ã¡ã¾ã™ã€‚ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã«ä»¥ä¸‹ã®ã‚ˆã†ãªè¡ŒãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¯ãšã§ã™ï¼š
```
INFO 2025-01-24 16:10:56 ts/train.py:263 Checkpoint policy after step 100
```
ã“ã“ã§ã€ãƒ—ãƒ­ã‚»ã‚¹ã‚’å¼·åˆ¶çµ‚äº†ã—ã¦ï¼ˆ`ctrl`+`c`ã‚’æŠ¼ã—ã¦ï¼‰ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ã¾ã—ã‚‡ã†ã€‚ãã®å¾Œã€ä»¥ä¸‹ã®ã‚³ãƒžãƒ³ãƒ‰ã§åˆ©ç”¨å¯èƒ½ãªæœ€å¾Œã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å®Ÿè¡Œ

ï¼ï¼ï¼ï¼ï¼ï¼


This tutorial will explain the training script, how to use it, and particularly how to configure everything needed for the training run.
> **Note:** The following assume you're running these commands on a machine equipped with a cuda GPU. If you don't have one (or if you're using a Mac), you can add `--device=cpu` (`--device=mps` respectively). However, be advised that the code executes much slower on cpu.


## The training script

LeRobot offers a training script at [`lerobot/scripts/train.py`](../../lerobot/scripts/train.py). At a high level it does the following:

- Initialize/load a configuration for the following steps using.
- Instantiates a dataset.
- (Optional) Instantiates a simulation environment corresponding to that dataset.
- Instantiates a policy.
- Runs a standard training loop with forward pass, backward pass, optimization step, and occasional logging, evaluation (of the policy on the environment), and checkpointing.

## Overview of the configuration system

In the training script, the main function `train` expects a `TrainPipelineConfig` object:
```python
# train.py
@parser.wrap()
def train(cfg: TrainPipelineConfig):
```

You can inspect the `TrainPipelineConfig` defined in [`lerobot/configs/train.py`](../../lerobot/configs/train.py) (which is heavily commented and meant to be a reference to understand any option)

When running the script, inputs for the command line are parsed thanks to the `@parser.wrap()` decorator and an instance of this class is automatically generated. Under the hood, this is done with [Draccus](https://github.com/dlwh/draccus) which is a tool dedicated for this purpose. If you're familiar with Hydra, Draccus can similarly load configurations from config files (.json, .yaml) and also override their values through command line inputs. Unlike Hydra, these configurations are pre-defined in the code through dataclasses rather than being defined entirely in config files. This allows for more rigorous serialization/deserialization, typing, and to manipulate configuration as objects directly in the code and not as dictionaries or namespaces (which enables nice features in an IDE such as autocomplete, jump-to-def, etc.)

Let's have a look at a simplified example. Amongst other attributes, the training config has the following attributes:
```python
@dataclass
class TrainPipelineConfig:
    dataset: DatasetConfig
    env: envs.EnvConfig | None = None
    policy: PreTrainedConfig | None = None
```
in which `DatasetConfig` for example is defined as such:
```python
@dataclass
class DatasetConfig:
    repo_id: str
    episodes: list[int] | None = None
    video_backend: str = "pyav"
```

This creates a hierarchical relationship where, for example assuming we have a `cfg` instance of `TrainPipelineConfig`, we can access the `repo_id` value with `cfg.dataset.repo_id`.
From the command line, we can specify this value with using a very similar syntax `--dataset.repo_id=repo/id`.

By default, every field takes its default value specified in the dataclass. If a field doesn't have a default value, it needs to be specified either from the command line or from a config file â€“ which path is also given in the command line (more in this below). In the example above, the `dataset` field doesn't have a default value which means it must be specified.


## Specifying values from the CLI

Let's say that we want to train [Diffusion Policy](../../lerobot/common/policies/diffusion) on the [pusht](https://huggingface.co/datasets/lerobot/pusht) dataset, using the [gym_pusht](https://github.com/huggingface/gym-pusht) environment for evaluation. The command to do so would look like this:
```bash
python lerobot/scripts/train.py \
    --dataset.repo_id=lerobot/pusht \
    --policy.type=diffusion \
    --env.type=pusht
```

Let's break this down:
- To specify the dataset, we just need to specify its `repo_id` on the hub which is the only required argument in the `DatasetConfig`. The rest of the fields have default values and in this case we are fine with those so we can just add the option `--dataset.repo_id=lerobot/pusht`.
- To specify the policy, we can just select diffusion policy using `--policy` appended with `.type`. Here, `.type` is a special argument which allows us to select config classes inheriting from `draccus.ChoiceRegistry` and that have been decorated with the `register_subclass()` method. To have a better explanation of this feature, have a look at this [Draccus demo](https://github.com/dlwh/draccus?tab=readme-ov-file#more-flexible-configuration-with-choice-types). In our code, we use this mechanism mainly to select policies, environments, robots, and some other components like optimizers. The policies available to select are located in [lerobot/common/policies](../../lerobot/common/policies)
- Similarly, we select the environment with `--env.type=pusht`. The different environment configs are available in [`lerobot/common/envs/configs.py`](../../lerobot/common/envs/configs.py)

Let's see another example. Let's say you've been training [ACT](../../lerobot/common/policies/act) on [lerobot/aloha_sim_insertion_human](https://huggingface.co/datasets/lerobot/aloha_sim_insertion_human) using the [gym-aloha](https://github.com/huggingface/gym-aloha) environment for evaluation with:
```bash
python lerobot/scripts/train.py \
    --policy.type=act \
    --dataset.repo_id=lerobot/aloha_sim_insertion_human \
    --env.type=aloha \
    --output_dir=outputs/train/act_aloha_insertion
```
> Notice we added `--output_dir` to explicitly tell where to write outputs from this run (checkpoints, training state, configs etc.). This is not mandatory and if you don't specify it, a default directory will be created from the current date and time, env.type and policy.type. This will typically look like `outputs/train/2025-01-24/16-10-05_aloha_act`.

We now want to train a different policy for aloha on another task. We'll change the dataset and use [lerobot/aloha_sim_transfer_cube_human](https://huggingface.co/datasets/lerobot/aloha_sim_transfer_cube_human) instead. Of course, we also need to change the task of the environment as well to match this other task.
Looking at the [`AlohaEnv`](../../lerobot/common/envs/configs.py) config, the task is `"AlohaInsertion-v0"` by default, which corresponds to the task we trained on in the command above. The [gym-aloha](https://github.com/huggingface/gym-aloha?tab=readme-ov-file#description) environment also has the `AlohaTransferCube-v0` task which corresponds to this other task we want to train on. Putting this together, we can train this new policy on this different task using:
```bash
python lerobot/scripts/train.py \
    --policy.type=act \
    --dataset.repo_id=lerobot/aloha_sim_transfer_cube_human \
    --env.type=aloha \
    --env.task=AlohaTransferCube-v0 \
    --output_dir=outputs/train/act_aloha_transfer
```

## Loading from a config file

Now, let's assume that we want to reproduce the run just above. That run has produced a `train_config.json` file in its checkpoints, which serializes the `TrainPipelineConfig` instance it used:
```json
{
    "dataset": {
        "repo_id": "lerobot/aloha_sim_transfer_cube_human",
        "episodes": null,
        ...
    },
    "env": {
        "type": "aloha",
        "task": "AlohaTransferCube-v0",
        "fps": 50,
        ...
    },
    "policy": {
        "type": "act",
        "n_obs_steps": 1,
        ...
    },
    ...
}
```

We can then simply load the config values from this file using:
```bash
python lerobot/scripts/train.py \
    --config_path=outputs/train/act_aloha_transfer/checkpoints/last/pretrained_model/ \
    --output_dir=outputs/train/act_aloha_transfer_2
```
`--config_path` is also a special argument which allows to initialize the config from a local config file. It can point to a directory that contains `train_config.json` or to the config file itself directly.

Similarly to Hydra, we can still override some parameters in the CLI if we want to, e.g.:
```bash
python lerobot/scripts/train.py \
    --config_path=outputs/train/act_aloha_transfer/checkpoints/last/pretrained_model/ \
    --output_dir=outputs/train/act_aloha_transfer_2
    --policy.n_action_steps=80
```
> Note: While `--output_dir` is not required in general, in this case we need to specify it since it will otherwise take the value from the `train_config.json` (which is `outputs/train/act_aloha_transfer`). In order to prevent accidental deletion of previous run checkpoints, we raise an error if you're trying to write in an existing directory. This is not the case when resuming a run, which is what you'll learn next.

`--config_path` can also accept the repo_id of a repo on the hub that contains a `train_config.json` file, e.g. running:
```bash
python lerobot/scripts/train.py --config_path=lerobot/diffusion_pusht
```
will start a training run with the same configuration used for training [lerobot/diffusion_pusht](https://huggingface.co/lerobot/diffusion_pusht)


## Resume training

Being able to resume a training run is important in case it crashed or aborted for any reason. We'll demonstrate how to that here.

Let's reuse the command from the previous run and add a few more options:
```bash
python lerobot/scripts/train.py \
    --policy.type=act \
    --dataset.repo_id=lerobot/aloha_sim_transfer_cube_human \
    --env.type=aloha \
    --env.task=AlohaTransferCube-v0 \
    --log_freq=25 \
    --save_freq=100 \
    --output_dir=outputs/train/run_resumption
```

Here we've taken care to set up the log frequency and checkpointing frequency to low numbers so we can showcase resumption. You should be able to see some logging and have a first checkpoint within 1 minute (depending on hardware). Wait for the first checkpoint to happen, you should see a line that looks like this in your terminal:
```
INFO 2025-01-24 16:10:56 ts/train.py:263 Checkpoint policy after step 100
```
Now let's simulate a crash by killing the process (hit `ctrl`+`c`). We can then simply resume this run from the last checkpoint available with:
```bash
python lerobot/scripts/train.py \
    --config_path=outputs/train/run_resumption/checkpoints/last/pretrained_model/ \
    --resume=true
```
You should see from the logging that your training picks up from where it left off.

Another reason for which you might want to resume a run is simply to extend training and add more training steps. The number of training steps is set by the option `--offline.steps`, which is 100 000 by default.
You could double the number of steps of the previous run with:
```bash
python lerobot/scripts/train.py \
    --config_path=outputs/train/run_resumption/checkpoints/last/pretrained_model/ \
    --resume=true \
    --offline.steps=200000
```

## Outputs of a run
In the output directory, there will be a folder called `checkpoints` with the following structure:
```bash
outputs/train/run_resumption/checkpoints
â”œâ”€â”€ 000100  # checkpoint_dir for training step 100
â”‚Â Â  â”œâ”€â”€ pretrained_model
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ config.json  # pretrained policy config
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ model.safetensors  # model weights
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ train_config.json  # train config
â”‚   â”‚   â””â”€â”€ README.md  # model card
â”‚Â Â  â””â”€â”€ training_state.pth  # optimizer/scheduler/rng state and training step
â”œâ”€â”€ 000200
â””â”€â”€ last -> 000200  # symlink to the last available checkpoint
```

## Fine-tuning a pre-trained policy

In addition to the features currently in Draccus, we've added a special `.path` argument for the policy, which allows to load a policy as you would with `PreTrainedPolicy.from_pretrained()`. In that case, `path` can be a local directory that contains a checkpoint or a repo_id pointing to a pretrained policy on the hub.

For example, we could fine-tune a [policy pre-trained on the aloha transfer task](https://huggingface.co/lerobot/act_aloha_sim_transfer_cube_human) on the aloha insertion task. We can achieve this with:
```bash
python lerobot/scripts/train.py \
    --policy.path=lerobot/act_aloha_sim_transfer_cube_human \
    --dataset.repo_id=lerobot/aloha_sim_insertion_human \
    --env.type=aloha \
    --env.task=AlohaInsertion-v0
```

When doing so, keep in mind that the features of the fine-tuning dataset would have to match the input/output features of the pretrained policy.

## Typical logs and metrics

When you start the training process, you will first see your full configuration being printed in the terminal. You can check it to make sure that you configured your run correctly. The final configuration will also be saved with the checkpoint.

After that, you will see training log like this one:
```
INFO 2024-08-14 13:35:12 ts/train.py:192 step:0 smpl:64 ep:1 epch:0.00 loss:1.112 grdn:15.387 lr:2.0e-07 updt_s:1.738 data_s:4.774
```
or evaluation log:
```
INFO 2024-08-14 13:38:45 ts/train.py:226 step:100 smpl:6K ep:52 epch:0.25 âˆ‘rwrd:20.693 success:0.0% eval_s:120.266
```

These logs will also be saved in wandb if `wandb.enable` is set to `true`. Here are the meaning of some abbreviations:
- `smpl`: number of samples seen during training.
- `ep`: number of episodes seen during training. An episode contains multiple samples in a complete manipulation task.
- `epch`: number of time all unique samples are seen (epoch).
- `grdn`: gradient norm.
- `âˆ‘rwrd`: compute the sum of rewards in every evaluation episode and then take an average of them.
- `success`: average success rate of eval episodes. Reward and success are usually different except for the sparsing reward setting, where reward=1 only when the task is completed successfully.
- `eval_s`: time to evaluate the policy in the environment, in second.
- `updt_s`: time to update the network parameters, in second.
- `data_s`: time to load a batch of data, in second.

Some metrics are useful for initial performance profiling. For example, if you find the current GPU utilization is low via the `nvidia-smi` command and `data_s` sometimes is too high, you may need to modify batch size or number of dataloading workers to accelerate dataloading. We also recommend [pytorch profiler](https://github.com/huggingface/lerobot?tab=readme-ov-file#improve-your-code-with-profiling) for detailed performance probing.

## In short

We'll summarize here the main use cases to remember from this tutorial.

#### Train a policy from scratch â€“ CLI
```bash
python lerobot/scripts/train.py \
    --policy.type=act \  # <- select 'act' policy
    --env.type=pusht \  # <- select 'pusht' environment
    --dataset.repo_id=lerobot/pusht  # <- train on this dataset
```

#### Train a policy from scratch - config file + CLI
```bash
python lerobot/scripts/train.py \
    --config_path=path/to/pretrained_model \  # <- can also be a repo_id
    --policy.n_action_steps=80  # <- you may still override values
```

#### Resume/continue a training run
```bash
python lerobot/scripts/train.py \
    --config_path=checkpoint/pretrained_model/ \
    --resume=true \
    --offline.steps=200000  # <- you can change some training parameters
```

#### Fine-tuning
```bash
python lerobot/scripts/train.py \
    --policy.path=lerobot/act_aloha_sim_transfer_cube_human \  # <- can also be a local path to a checkpoint
    --dataset.repo_id=lerobot/aloha_sim_insertion_human \
    --env.type=aloha \
    --env.task=AlohaInsertion-v0
```

---

Now that you know the basics of how to train a policy, you might want to know how to apply this knowledge to actual robots, or how to record your own datasets and train policies on your specific task?
If that's the case, head over to the next tutorial [`7_get_started_with_real_robot.md`](./7_get_started_with_real_robot.md).

Or in the meantime, happy training! ðŸ¤—
